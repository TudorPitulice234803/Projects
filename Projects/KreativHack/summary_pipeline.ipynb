{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532874a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "print(wikipedia.summary(\"Python (programming language)\", sentences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd88f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (main, Aug 15 2025, 12:34:02) [GCC 15.2.1 20250813]\n",
      "Torch:  2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1660 Ti\n",
      "Compute capability: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Torch:  {torch.__version__}\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dec2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Opole Zoo', 'Opole Cathedral', 'Opole Główne railway station', 'Opole-Kamień Śląski Airport', 'Opole', 'New Synagogue (Opole)', 'Opole Voivodeship', 'Opole Town Hall', 'Moszna, Opole Voivodeship', 'Dębowiec, Opole Voivodeship', 'Czyżowice, Opole Voivodeship', 'Tułowice, Opole Voivodeship', 'Biała, Opole Voivodeship', 'Jaryszów, Opole Voivodeship', 'Opole University of Technology', 'Lubomirski Palace (Opole Lubelskie)', 'Opole Lubelskie', 'Vladislaus II of Opole', 'Diocese of Opole', 'Casimir I of Opole', 'Kępa, Opole Voivodeship', 'Przechód, Opole Voivodeship', 'Vladislaus I of Opole', 'Bolko I of Opole', 'Bolko IV of Opole', 'Prędocin, Opole Voivodeship', 'Duchy of Opole and Racibórz', 'Duchy of Opole', 'Smolarnia, Opole Voivodeship', 'Markowice, Opole Voivodeship', 'National Festival of Polish Song in Opole']\n",
      "\n",
      "==== CITY ====\n",
      " Opole\n",
      "\n",
      "==== VISITED SITES ====\n",
      " - Casimir I of Opole\n",
      " - Opole University of Technology\n",
      " - Czyżowice, Opole Voivodeship\n",
      " - Bolko IV of Opole\n",
      " - Duchy of Opole\n",
      "\n",
      "==== SUMMARY ====\n",
      " September 18, 2025  Opole City Tour  Casimir I of Opole: A Piast dynasty duke who ruled Opole and\n",
      "Racibórz from 1211 until his death in 1230. Born to Duke Mieszko I Tanglefoot and Ludmilla, he was\n",
      "instrumental in the growth of the Silesian lands, seizing Opole from his uncle Bolesław I in 1201.\n",
      "In 1215, Casimir granted significant privileges to the church, leading to the formation of the semi-\n",
      "independent district of Ujazd.  Opole University of Technology: Founded in 1959, this university\n",
      "offers courses in seven faculties, including Civil Engineering and Architecture, Mechanical\n",
      "Engineering, Electrical Engineering, and Production Engineering. The university has over 500\n",
      "lecturers and over 9,000 students, and in 2008 it opened the Opole Confucius Institute to promote\n",
      "Chinese language and culture.  Czyżowice: A picturesque village near Prudnik, Czyżowice features a\n",
      "chapel dating back\n",
      "September 18, 2025\n",
      "\n",
      "Opole City Tour\n",
      "\n",
      "Casimir I of Opole: A Piast dynasty duke who ruled Opole and Racibórz from 1211 until his death in 1230. Born to Duke Mieszko I Tanglefoot and Ludmilla, he was instrumental in the growth of the Silesian lands, seizing Opole from his uncle Bolesław I in 1201. In 1215, Casimir granted significant privileges to the church, leading to the formation of the semi-independent district of Ujazd.\n",
      "\n",
      "Opole University of Technology: Founded in 1959, this university offers courses in seven faculties, including Civil Engineering and Architecture, Mechanical Engineering, Electrical Engineering, and Production Engineering. The university has over 500 lecturers and over 9,000 students, and in 2008 it opened the Opole Confucius Institute to promote Chinese language and culture.\n",
      "\n",
      "Czyżowice: A picturesque village near Prudnik, Czyżowice features a chapel dating back\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "City trip diary summarizer using:\n",
    "- wikipedia Python library (no manual HTTP calls)\n",
    "- Ollama local LLM server with the 'phi3:mini' model\n",
    "\n",
    "Quick start (terminal):\n",
    "  pip install wikipedia ollama\n",
    "\n",
    "Ollama:\n",
    "  - Ensure the local server is running (http://localhost:11434)\n",
    "  - Pull model:  ollama pull phi3:mini\n",
    "\n",
    "Notebook-friendly:\n",
    "  - Call run_city_diary(CITY=\"Opole\", NUM_SITES=3) directly in a cell\n",
    "  - Or pass SPECIFIC_LOCATIONS=[\"Opole Town Hall\", ...] to lock pages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import random\n",
    "import textwrap\n",
    "from datetime import date\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import wikipedia\n",
    "import ollama  # pip install ollama\n",
    "\n",
    "# -------------------- Defaults (easy to tweak) --------------------\n",
    "MODEL_NAME = \"mistral:7b\"    # change to any local Ollama model you like\n",
    "WIKI_LANG = \"en\"\n",
    "MAX_ARTICLE_CHARS = 2400    # truncate Wikipedia article text per page\n",
    "MAX_NEW_TOKENS = 256        # LLM budget for the final diary entry\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "\n",
    "# Seed for reproducibility (optional)\n",
    "random.seed(42)\n",
    "wikipedia.set_lang(WIKI_LANG)\n",
    "\n",
    "\n",
    "# -------------------- Wikipedia helpers --------------------\n",
    "def _is_probably_article(title: str) -> bool:\n",
    "    \"\"\"Filter out likely non-article titles (lists, categories, templates).\"\"\"\n",
    "    t = title.strip().lower()\n",
    "    if t.startswith((\"category:\", \"template:\")) or t.startswith(\"list of\"):\n",
    "        return False\n",
    "    if \"disambiguation\" in t:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def search_city_candidates(city: str, max_results_per_query: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build a diverse candidate pool of likely POIs (articles) for the given city.\n",
    "    \"\"\"\n",
    "    city_lc = city.strip().lower()\n",
    "    queries = [\n",
    "        f\"Buildings and structures in {city}\",\n",
    "        f\"{city} monuments\",\n",
    "        f\"{city} landmarks\",\n",
    "        f\"{city} architecture\",\n",
    "        f\"{city} church\",\n",
    "        f\"{city} tower\",\n",
    "        f\"{city} museum\",\n",
    "        f\"{city} amphitheatre\",\n",
    "        f\"{city} historic sites\",\n",
    "    ]\n",
    "\n",
    "    candidates: List[str] = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            results = wikipedia.search(q, results=max_results_per_query)\n",
    "            for r in results:\n",
    "                if _is_probably_article(r) and city_lc in r.lower():\n",
    "                    candidates.append(r)\n",
    "        except Exception:\n",
    "            # Ignore individual query failures and keep going\n",
    "            pass\n",
    "\n",
    "    # De-duplicate while preserving order\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            unique.append(c)\n",
    "            seen.add(c)\n",
    "    print(unique)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def fetch_wikipedia_extract(title: str, max_chars: int = MAX_ARTICLE_CHARS) -> str:\n",
    "    \"\"\"\n",
    "    Get clean article content for a title, with basic disambiguation handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(title, auto_suggest=False, preload=False)\n",
    "        text = page.content or \"\"\n",
    "        return text[:max_chars]\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # Choose a random option and recurse\n",
    "        options = [opt for opt in e.options if _is_probably_article(opt)]\n",
    "        if not options:\n",
    "            options = e.options\n",
    "        new_title = random.choice(options)\n",
    "        return fetch_wikipedia_extract(new_title, max_chars=max_chars)\n",
    "    except wikipedia.PageError:\n",
    "        # Try a fallback suggestion from search\n",
    "        try:\n",
    "            alt = wikipedia.search(title)\n",
    "            if alt:\n",
    "                return fetch_wikipedia_extract(alt[0], max_chars=max_chars)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        # Any unexpected error -> empty\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def choose_sites(\n",
    "    city: str,\n",
    "    num_sites: int,\n",
    "    specific_locations: Optional[Iterable[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Decide which site article titles to use.\n",
    "    - If specific_locations are provided, use up to num_sites of those (in given order).\n",
    "    - Else pick randomly from search results; if empty, fall back to a curated list per city.\n",
    "    \"\"\"\n",
    "    if specific_locations:\n",
    "        titles = [t for t in specific_locations if isinstance(t, str) and t.strip()]\n",
    "        return titles[:num_sites]\n",
    "\n",
    "    pool = search_city_candidates(city)\n",
    "\n",
    "    if not pool:\n",
    "        raise RuntimeError(\n",
    "            f\"No candidate articles found for city '{city}'. \"\n",
    "            \"Try providing SPECIFIC_LOCATIONS.\"\n",
    "        )\n",
    "\n",
    "    random.shuffle(pool)\n",
    "    return pool[:num_sites]\n",
    "\n",
    "\n",
    "# -------------------- Prompting / LLM --------------------\n",
    "def build_messages_for_trip(\n",
    "    city: str,\n",
    "    sites_and_texts: List[Tuple[str, str]],\n",
    "    style: str = \"diary\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a system+user message pair for Ollama chat, injecting current date.\n",
    "    style: \"diary\" or \"bullets\" (feel free to add more styles if you like)\n",
    "    \"\"\"\n",
    "    today = date.today().strftime(\"%B %d, %Y\")\n",
    "\n",
    "    if style == \"diary\":\n",
    "        system = (\n",
    "            \"You are a concise diary-writing assistant for tourists. \"\n",
    "            \"The current date is {today}. Write very concisely, but cover all visited sites\"\n",
    "        )\n",
    "        instruction = (\n",
    "            \"Write a diary-like entry that summarizes a guided city trip in {city}. \"\n",
    "            \"The group visited the following sites today. For each site, capture what makes it unique \"\n",
    "            \"(key history, architecture, dates, measurements, notable facts). Avoid fluff; keep it tight.\"\n",
    "        )\n",
    "    else:  # \"bullets\"\n",
    "        system = (\n",
    "            \"You are a concise, factual assistant. The current date is {today}. \"\n",
    "            \"Summarize clearly in bullet points.\"\n",
    "        )\n",
    "        instruction = (\n",
    "            \"Summarize a guided city trip in {city} as ~6-10 concise bullet points. \"\n",
    "            \"For each visited site include key dates, names, measurements, or counts where relevant.\"\n",
    "        )\n",
    "\n",
    "    # Join site texts\n",
    "    parts = []\n",
    "    for title, content in sites_and_texts:\n",
    "        parts.append(f\"### {title}\\n{content}\\n\")\n",
    "    sites_block = \"\\n\".join(parts)\n",
    "\n",
    "    user = (\n",
    "        instruction.format(city=city) +\n",
    "        \"\\n\\nVisited sites (Wikipedia excerpts, possibly truncated):\\n\\n\" +\n",
    "        sites_block\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system.format(today=today)},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_with_ollama(\n",
    "    model: str,\n",
    "    messages: List[dict],\n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    "    repeat_penalty: float = 1.05,\n",
    "    stream: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call the local Ollama server using the Python client.\n",
    "    \"\"\"\n",
    "    # Allow overriding OLLAMA_HOST via env; ollama lib picks it up automatically.\n",
    "    # (If you're pointing to a remote host, set OLLAMA_HOST before importing ollama.)\n",
    "    _ = ollama.list()  # quick connectivity check; raises if server is down\n",
    "\n",
    "    resp = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options={\n",
    "            \"num_predict\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"repeat_penalty\": repeat_penalty,\n",
    "        },\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        chunks = []\n",
    "        for chunk in resp:\n",
    "            if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                chunks.append(chunk[\"message\"][\"content\"])\n",
    "        return \"\".join(chunks).strip()\n",
    "\n",
    "    return resp[\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "# -------------------- Main entry (Notebook-friendly) --------------------\n",
    "def run_city_diary(\n",
    "    CITY: str = \"Opole\",\n",
    "    NUM_SITES: int = 5,\n",
    "    SPECIFIC_LOCATIONS: Optional[Iterable[str]] = None,\n",
    "    MODEL: str = MODEL_NAME,\n",
    "    STYLE: str = \"diary\",\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[List[str], str]:\n",
    "    \"\"\"\n",
    "    Orchestrates the full flow:\n",
    "      1) Pick site titles (from SPECIFIC_LOCATIONS or search/fallback)\n",
    "      2) Fetch Wikipedia excerpts for each site\n",
    "      3) Ask the model to produce a diary/bullets summary\n",
    "\n",
    "    Returns: (selected_titles, summary_text)\n",
    "    \"\"\"\n",
    "    # pick sites\n",
    "    titles = choose_sites(CITY, NUM_SITES, SPECIFIC_LOCATIONS)\n",
    "\n",
    "    # fetch extracts\n",
    "    sites_and_texts: List[Tuple[str, str]] = []\n",
    "    for t in titles:\n",
    "        txt = fetch_wikipedia_extract(t, max_chars=MAX_ARTICLE_CHARS)\n",
    "        if not txt.strip():\n",
    "            # skip empty pages; try to keep count at least >0\n",
    "            if verbose:\n",
    "                print(f\"[warn] Empty content for '{t}', skipping.\")\n",
    "            continue\n",
    "        sites_and_texts.append((t, txt))\n",
    "\n",
    "    if not sites_and_texts:\n",
    "        raise RuntimeError(\"No article content retrieved. Try different SPECIFIC_LOCATIONS or city.\")\n",
    "\n",
    "    # build messages & generate\n",
    "    messages = build_messages_for_trip(CITY, sites_and_texts, style=STYLE)\n",
    "    summary = generate_with_ollama(MODEL, messages, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n==== CITY ====\\n\", CITY)\n",
    "        print(\"\\n==== VISITED SITES ====\")\n",
    "        for t, _ in sites_and_texts:\n",
    "            print(\" -\", t)\n",
    "        print(\"\\n==== SUMMARY ====\\n\", textwrap.fill(summary, 100))\n",
    "\n",
    "    # Return only the titles we actually used (with content)\n",
    "    used_titles = [t for t, _ in sites_and_texts]\n",
    "    return used_titles, summary\n",
    "\n",
    "\n",
    "# -------------------- Example usage --------------------\n",
    "\n",
    "titles, diary = run_city_diary(\n",
    "     CITY=\"Opole\",\n",
    "     NUM_SITES=5,\n",
    "     SPECIFIC_LOCATIONS=None,  # or [\"Opole Town Hall\", \"Piast Tower (Opole)\"]\n",
    "     MODEL=\"mistral:7b\",\n",
    "     STYLE=\"diary\",            # or \"bullets\"\n",
    "     verbose=True,\n",
    " )\n",
    "print(diary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f720b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RANDOM OPOLE MONUMENT ====\n",
      " New Synagogue (Opole)\n",
      "\n",
      "==== SUMMARY ====\n",
      " September 18, 2025: Visited the hauntingly beautiful ruins of the New Synagogue in Opole, now\n",
      "Poland. Founded as a Reform Jewish congregation and built between 1893-1897 by Felix Henry, it was a\n",
      "striking Moorish Revival style structure that replaced the Old Synagogue dating back to 1841.\n",
      "However, on November 9, 1938, during Kristallnacht, this symbol of Jewish faith and community was\n",
      "mercilessly destroyed by Nazis under the orders of rabbi Hans Hirschberg. Standing amidst its ashes\n",
      "today is a stark reminder of the atrocities of the Holoca0023-165473-209807-5  The New Synagogue was\n",
      "not only a place of worship but also an integral part of Jewish life and culture in Opole. It's\n",
      "heartbreaking to see such a significant site reduced to rubble, yet the resilience of the spirit is\n",
      "evident as the Old Synagogue still stands nearby, repurposed for commercial use since 1897.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Random Opole monument summarizer using:\n",
    "- wikipedia Python library (no manual HTTP calls)\n",
    "- Ollama local LLM server with the 'phi3:mini' model\n",
    "\n",
    "Requires:\n",
    "  - Ollama running locally (http://localhost:11434)\n",
    "  - Model pulled: `ollama pull phi3:mini`\n",
    "  - Python deps: `pip install wikipedia ollama`\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import textwrap\n",
    "import wikipedia\n",
    "from datetime import date\n",
    "\n",
    "# If you run Ollama on a different host/port, set OLLAMA_HOST, e.g.:\n",
    "# os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
    "\n",
    "import ollama  # Python client for Ollama\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "MODEL_NAME = \"phi3:mini\"\n",
    "WIKI_LANG = \"en\"\n",
    "MAX_ARTICLE_CHARS = 2400\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "# Seed for reproducibility across runs (optional)\n",
    "random.seed(42)\n",
    "\n",
    "# -------------------- Wikipedia helpers --------------------\n",
    "wikipedia.set_lang(WIKI_LANG)\n",
    "\n",
    "_FALLBACK_OPOLE_TITLES = [\n",
    "    \"Opole Town Hall\",\n",
    "    \"Cathedral of the Holy Cross, Opole\",\n",
    "    \"Piast Tower (Opole)\",\n",
    "    \"Amphitheatre Tysiąclecia in Opole\",\n",
    "    \"Church of the Exaltation of the Holy Cross, Opole\",\n",
    "    \"Museum of Opole Silesia\",\n",
    "]\n",
    "\n",
    "def is_probably_article(title: str) -> bool:\n",
    "    \"\"\"Filter out likely non-article titles (lists, categories, templates).\"\"\"\n",
    "    t = title.strip().lower()\n",
    "    if t.startswith(\"category:\") or t.startswith(\"list of\") or t.startswith(\"template:\"):\n",
    "        return False\n",
    "    if \"disambiguation\" in t:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_random_opole_monument() -> str:\n",
    "    \"\"\"\n",
    "    Use wikipedia.search with several queries likely to surface actual buildings/structures in Opole.\n",
    "    If that fails, fall back to a small curated list.\n",
    "    \"\"\"\n",
    "    queries = [\n",
    "        \"Buildings and structures in Opole\",\n",
    "        \"Opole landmarks\",\n",
    "        \"Opole architecture\",\n",
    "        \"Opole church\",\n",
    "        \"Opole tower\",\n",
    "        \"Opole museum\",\n",
    "        \"Opole amphitheatre\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            results = wikipedia.search(q)\n",
    "            for r in results:\n",
    "                if is_probably_article(r) and \"opole\" in r.lower():\n",
    "                    candidates.append(r)\n",
    "        except Exception:\n",
    "            # Ignore and try next query\n",
    "            pass\n",
    "\n",
    "    # Unique & filtered\n",
    "    candidates = [c for i, c in enumerate(candidates) if c not in candidates[:i]]\n",
    "\n",
    "    if not candidates:\n",
    "        candidates = _FALLBACK_OPOLE_TITLES[:]\n",
    "\n",
    "    return random.choice(candidates)\n",
    "\n",
    "def fetch_wikipedia_extract(title: str, max_chars: int = MAX_ARTICLE_CHARS) -> str:\n",
    "    \"\"\"\n",
    "    Get clean article content for a title, with basic disambiguation handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(title, auto_suggest=False, preload=False)\n",
    "        text = page.content or \"\"\n",
    "        return text[:max_chars]\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # Choose a random option and recurse\n",
    "        options = [opt for opt in e.options if is_probably_article(opt)]\n",
    "        if not options:\n",
    "            options = e.options\n",
    "        new_title = random.choice(options)\n",
    "        return fetch_wikipedia_extract(new_title, max_chars=max_chars)\n",
    "    except wikipedia.PageError:\n",
    "        # Try a fallback suggestion from search\n",
    "        try:\n",
    "            alt = wikipedia.search(title)\n",
    "            if alt:\n",
    "                return fetch_wikipedia_extract(alt[0], max_chars=max_chars)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        # Any unexpected error -> empty\n",
    "        return \"\"\n",
    "\n",
    "# -------------------- Prompting --------------------\n",
    "def build_phi_style_messages(page_title: str, page_text: str):\n",
    "    \"\"\"\n",
    "    Use Ollama's chat format (system + user messages).\n",
    "    Phi-style models work well with a simple, explicit instruction.\n",
    "    \"\"\"\n",
    "    today = date.today().strftime(\"%B %d, %Y\")\n",
    "    system = f\"You are a concise diary writing assistant, responsible for a memorable note for tourists about their city trip today {today}.\"\n",
    "    user = (\n",
    "        \"Write a dairy-like entry that summarizes a city trip that tourists made with a tourguide\"\n",
    "        \"past the following sites mentioned in the Wikipedia article below. \"\n",
    "        \"Icnclude key details why it is a unique site. \"\n",
    "        \"Avoid fluff.\\n\\n\"\n",
    "        f\"Article title: {page_title}\\n\\n\"\n",
    "        f\"Article text:\\n{page_text}\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "def summarize_with_ollama(messages, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"\n",
    "    Call the local Ollama server using the Python client.\n",
    "    \"\"\"\n",
    "    resp = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        options={\n",
    "            \"num_predict\": max_new_tokens,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repeat_penalty\": 1.05,\n",
    "        },\n",
    "        stream=False,  # set True to stream tokens; then iterate resp as a generator\n",
    "    )\n",
    "    # resp looks like {\"model\": ..., \"message\": {\"role\": \"assistant\", \"content\": \"...\"} , ...}\n",
    "    return resp[\"message\"][\"content\"].strip()\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    # Quick connectivity check (optional): list models; will raise if server is down\n",
    "    try:\n",
    "        _ = ollama.list()\n",
    "    except Exception as e:\n",
    "        raise SystemExit(\n",
    "            \"Could not reach the local Ollama server at http://localhost:11434.\\n\"\n",
    "            \"Start it and ensure the model is pulled:\\n\"\n",
    "            \"  1) ollama serve  (if not already running)\\n\"\n",
    "            \"  2) ollama pull phi3:mini\\n\"\n",
    "            f\"Error: {e}\"\n",
    "        )\n",
    "\n",
    "    title = get_random_opole_monument()\n",
    "    raw = fetch_wikipedia_extract(title, max_chars=MAX_ARTICLE_CHARS)\n",
    "\n",
    "    if not raw.strip():\n",
    "        raise RuntimeError(f\"Could not fetch article content for '{title}'.\")\n",
    "\n",
    "    messages = build_phi_style_messages(title, raw)\n",
    "    summary = summarize_with_ollama(messages, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    print(\"\\n==== RANDOM OPOLE MONUMENT ====\\n\", title)\n",
    "    print(\"\\n==== SUMMARY ====\\n\", textwrap.fill(summary, 100))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5cef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Wikipedia:  14%|█▍        | 42/303 [00:43<05:09,  1.19s/it]c:\\Users\\tudor\\anaconda3\\envs\\KreativHack\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\tudor\\anaconda3\\envs\\KreativHack\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Checking Wikipedia: 100%|██████████| 303/303 [06:39<00:00,  1.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_monument</th>\n",
       "      <th>city</th>\n",
       "      <th>number_reviews</th>\n",
       "      <th>wheelchair_accesability</th>\n",
       "      <th>wiki_lang</th>\n",
       "      <th>found</th>\n",
       "      <th>match_title</th>\n",
       "      <th>url</th>\n",
       "      <th>method</th>\n",
       "      <th>note</th>\n",
       "      <th>pageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rijksmuseum</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>106747</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>Rijksmuseum</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rijksmuseum</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>26230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vondelpark</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>57536</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>Vondelpark</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vondelpark</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>3231819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dam Square</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>44864</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>Dam Square</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dam_Square</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>638647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEMO Science Museum</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>35107</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>List of tourist attractions in Amsterdam</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_tourist_...</td>\n",
       "      <td>search</td>\n",
       "      <td>NEMO Science Museum Amsterdam</td>\n",
       "      <td>5024484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTIS</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>34963</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>Wereldmuseum Amsterdam</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wereldmuseum_Ams...</td>\n",
       "      <td>search</td>\n",
       "      <td>ARTIS Amsterdam</td>\n",
       "      <td>7148085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name_monument       city  number_reviews  wheelchair_accesability  \\\n",
       "0          Rijksmuseum  Amsterdam          106747                     True   \n",
       "1           Vondelpark  Amsterdam           57536                     True   \n",
       "2           Dam Square  Amsterdam           44864                     True   \n",
       "3  NEMO Science Museum  Amsterdam           35107                     True   \n",
       "4                ARTIS  Amsterdam           34963                     True   \n",
       "\n",
       "  wiki_lang  found                               match_title  \\\n",
       "0        en   True                               Rijksmuseum   \n",
       "1        en   True                                Vondelpark   \n",
       "2        en   True                                Dam Square   \n",
       "3        en   True  List of tourist attractions in Amsterdam   \n",
       "4        en   True                    Wereldmuseum Amsterdam   \n",
       "\n",
       "                                                 url  method  \\\n",
       "0          https://en.wikipedia.org/wiki/Rijksmuseum   title   \n",
       "1           https://en.wikipedia.org/wiki/Vondelpark   title   \n",
       "2           https://en.wikipedia.org/wiki/Dam_Square   title   \n",
       "3  https://en.wikipedia.org/wiki/List_of_tourist_...  search   \n",
       "4  https://en.wikipedia.org/wiki/Wereldmuseum_Ams...  search   \n",
       "\n",
       "                            note   pageid  \n",
       "0                          exact    26230  \n",
       "1                          exact  3231819  \n",
       "2                          exact   638647  \n",
       "3  NEMO Science Museum Amsterdam  5024484  \n",
       "4                ARTIS Amsterdam  7148085  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setup: pip install wikipedia if needed ---\n",
    "# %pip install wikipedia pandas tqdm\n",
    "\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import wikipedia\n",
    "\n",
    "def _try_get_page(title: str) -> Tuple[Optional[wikipedia.WikipediaPage], str]:\n",
    "    \"\"\"Attempt an exact page fetch without autosuggest; return (page_or_None, note).\"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(title, auto_suggest=False, preload=False)\n",
    "        return page, \"exact\"\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        # Heuristic: prefer options that mention the city in parentheses or after a comma\n",
    "        return None, f\"disambiguation:{len(e.options)}\"\n",
    "    except wikipedia.PageError:\n",
    "        return None, \"not_found\"\n",
    "    except Exception as e:\n",
    "        return None, f\"error:{type(e).__name__}\"\n",
    "\n",
    "def _resolve_disambiguation(options: List[str], city: str, name: str) -> List[str]:\n",
    "    \"\"\"Order disambiguation options by how well they match the intended page.\"\"\"\n",
    "    city_l = city.lower()\n",
    "    name_l = name.lower()\n",
    "    scored = []\n",
    "    for opt in options:\n",
    "        o_l = opt.lower()\n",
    "        score = 0\n",
    "        if city_l in o_l: score += 3\n",
    "        if name_l in o_l: score += 2\n",
    "        # parentheses with city often indicate the right one\n",
    "        if f\"({city_l})\" in o_l: score += 1\n",
    "        scored.append((score, opt))\n",
    "    scored.sort(reverse=True)\n",
    "    return [opt for _, opt in scored]\n",
    "\n",
    "def _search_titles(query: str, max_results: int = 20) -> List[str]:\n",
    "    try:\n",
    "        return wikipedia.search(query, results=max_results) or []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def find_wikipedia_for_monument(\n",
    "    name_monument: str,\n",
    "    city: str,\n",
    "    languages: List[str] = (\"en\",),\n",
    "    polite_delay: float = 0.2,\n",
    "    max_search_results: int = 20,\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Try to find a Wikipedia article for a given (monument, city).\n",
    "    Tries multiple languages in order; returns metadata about the best match found.\n",
    "    \"\"\"\n",
    "    name = (name_monument or \"\").strip()\n",
    "    cty  = (city or \"\").strip()\n",
    "    if not name or not cty:\n",
    "        return {\n",
    "            \"wiki_lang\": None, \"found\": False, \"match_title\": None, \"url\": None,\n",
    "            \"method\": None, \"note\": \"missing_name_or_city\", \"pageid\": None\n",
    "        }\n",
    "\n",
    "    # Title candidates (common patterns)\n",
    "    title_candidates = [\n",
    "        name,                                     # \"Piast Tower\"\n",
    "        f\"{name} ({cty})\",                        # \"Cathedral ... (Opole)\"\n",
    "        f\"{name}, {cty}\",                         # \"Town Hall, Opole\"\n",
    "        f\"{name} in {cty}\",                       # \"Amphitheatre ... in Opole\"\n",
    "    ]\n",
    "\n",
    "    # Search queries\n",
    "    search_queries = [\n",
    "        f\"{name} {cty}\",\n",
    "        f\"{name} {cty} site\",\n",
    "        f\"{name} {cty} landmark\",\n",
    "        f\"{name} {cty} building\",\n",
    "    ]\n",
    "\n",
    "    for lang in languages:\n",
    "        wikipedia.set_lang(lang)\n",
    "        # 1) Try exact-ish candidates first\n",
    "        for title in title_candidates:\n",
    "            page, note = _try_get_page(title)\n",
    "            if page:\n",
    "                return {\n",
    "                    \"wiki_lang\": lang, \"found\": True, \"match_title\": page.title,\n",
    "                    \"url\": page.url, \"method\": \"title\", \"note\": note, \"pageid\": page.pageid\n",
    "                }\n",
    "            if note.startswith(\"error\"):\n",
    "                # transient issues: brief delay and continue\n",
    "                time.sleep(polite_delay)\n",
    "\n",
    "        # 2) Search and then try best-looking results\n",
    "        for q in search_queries:\n",
    "            results = _search_titles(q, max_results=max_search_results)\n",
    "            # Prefer titles containing the city or the full name\n",
    "            ranked = []\n",
    "            c_l = cty.lower(); n_l = name.lower()\n",
    "            for r in results:\n",
    "                r_l = r.lower()\n",
    "                score = 0\n",
    "                if c_l in r_l: score += 2\n",
    "                if n_l in r_l: score += 1\n",
    "                ranked.append((score, r))\n",
    "            ranked.sort(reverse=True)\n",
    "            for _, rtitle in ranked:\n",
    "                try:\n",
    "                    page = wikipedia.page(rtitle, auto_suggest=False, preload=False)\n",
    "                    return {\n",
    "                        \"wiki_lang\": lang, \"found\": True, \"match_title\": page.title,\n",
    "                        \"url\": page.url, \"method\": \"search\", \"note\": q, \"pageid\": page.pageid\n",
    "                    }\n",
    "                except wikipedia.DisambiguationError as e:\n",
    "                    for opt in _resolve_disambiguation(e.options, cty, name):\n",
    "                        try:\n",
    "                            page = wikipedia.page(opt, auto_suggest=False, preload=False)\n",
    "                            return {\n",
    "                                \"wiki_lang\": lang, \"found\": True, \"match_title\": page.title,\n",
    "                                \"url\": page.url, \"method\": \"search_disamb\", \"note\": q, \"pageid\": page.pageid\n",
    "                            }\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                except Exception:\n",
    "                    continue\n",
    "            time.sleep(polite_delay)\n",
    "\n",
    "    # Nothing found in any language\n",
    "    return {\n",
    "        \"wiki_lang\": None, \"found\": False, \"match_title\": None, \"url\": None,\n",
    "        \"method\": None, \"note\": \"not_found_any_lang\", \"pageid\": None\n",
    "    }\n",
    "\n",
    "def annotate_csv_with_wikipedia(\n",
    "    csv_path: str,\n",
    "    out_path: Optional[str] = None,\n",
    "    languages: List[str] = (\"en\", \"pl\"),\n",
    "    polite_delay: float = 0.2,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV with columns: name_monument, city.\n",
    "    Adds columns describing the match found on Wikipedia.\n",
    "    Saves to out_path if provided; returns the annotated DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = {\"name_monument\", \"city\", \"number_reviews\", \"wheelchair_accesability\"}\n",
    "    missing = required - set(df.columns.str.lower())\n",
    "    # Try case-insensitive mapping\n",
    "    col_map = {c.lower(): c for c in df.columns}\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV must contain columns {required}; got {list(df.columns)}\")\n",
    "\n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Checking Wikipedia\"):\n",
    "        name = row[col_map.get(\"name_monument\", \"name_monument\")]\n",
    "        city = row[col_map.get(\"city\", \"city\")]\n",
    "        meta = find_wikipedia_for_monument(\n",
    "            name_monument=name,\n",
    "            city=city,\n",
    "            languages=languages,\n",
    "            polite_delay=polite_delay\n",
    "        )\n",
    "        results.append(meta)\n",
    "\n",
    "    meta_df = pd.DataFrame(results)\n",
    "    out_df = pd.concat([df.reset_index(drop=True), meta_df], axis=1)\n",
    "\n",
    "    if out_path:\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "csv_in  = \"cultural_demoset.csv\"\n",
    "csv_out = \"wiki_cultural_demoset.csv\"\n",
    "\n",
    "annotated = annotate_csv_with_wikipedia(\n",
    "    csv_path=csv_in,\n",
    "    out_path=csv_out,         # or None to skip saving\n",
    "    languages=[\"en\"],   # try English then Polish\n",
    "    polite_delay=0.2\n",
    ")\n",
    "\n",
    "annotated.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1d8900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "print(len(annotated[annotated[\"method\"] == \"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06da4986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching article text for 124 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [01:24<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding query: 'Art'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>match_title</th>\n",
       "      <th>wiki_lang</th>\n",
       "      <th>url</th>\n",
       "      <th>name_monument</th>\n",
       "      <th>city</th>\n",
       "      <th>method</th>\n",
       "      <th>note</th>\n",
       "      <th>pageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316342</td>\n",
       "      <td>Museum of Art Collections</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Museum_of_Art_Co...</td>\n",
       "      <td>Museum of Art Collections</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>3201895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.301075</td>\n",
       "      <td>Museum of Decorative Arts in Prague</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Museum_of_Decora...</td>\n",
       "      <td>Museum of Decorative Arts in Prague</td>\n",
       "      <td>Prague</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>23548614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.292459</td>\n",
       "      <td>Hungarian National Gallery</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hungarian_Nation...</td>\n",
       "      <td>Hungarian National Gallery</td>\n",
       "      <td>Budapest</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>7645878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270278</td>\n",
       "      <td>Musée d'Art Moderne de Paris</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mus%C3%A9e_d%27A...</td>\n",
       "      <td>Musée d'Art Moderne de Paris</td>\n",
       "      <td>Paris</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>15614518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.262694</td>\n",
       "      <td>Stedelijk Museum Amsterdam</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stedelijk_Museum...</td>\n",
       "      <td>Stedelijk Museum Amsterdam</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>931845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.260603</td>\n",
       "      <td>Little Princess statue</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Little_Princess_...</td>\n",
       "      <td>Little Princess Statue</td>\n",
       "      <td>Budapest</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>38595315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.254313</td>\n",
       "      <td>Palais de la Porte Dorée</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Palais_de_la_Por...</td>\n",
       "      <td>Palais de la Porte Dorée</td>\n",
       "      <td>Paris</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>17470851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.252783</td>\n",
       "      <td>Vatican Museums</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vatican_Museums</td>\n",
       "      <td>Vatican Museums</td>\n",
       "      <td>Rome</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>229765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.252783</td>\n",
       "      <td>Vatican Museums</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vatican_Museums</td>\n",
       "      <td>Museo Gregoriano Profano</td>\n",
       "      <td>Rome</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>229765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.252783</td>\n",
       "      <td>Vatican Museums</td>\n",
       "      <td>en</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vatican_Museums</td>\n",
       "      <td>Braccio Nuovo</td>\n",
       "      <td>Rome</td>\n",
       "      <td>title</td>\n",
       "      <td>exact</td>\n",
       "      <td>229765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similarity                          match_title wiki_lang  \\\n",
       "0    0.316342            Museum of Art Collections        en   \n",
       "1    0.301075  Museum of Decorative Arts in Prague        en   \n",
       "2    0.292459           Hungarian National Gallery        en   \n",
       "3    0.270278         Musée d'Art Moderne de Paris        en   \n",
       "4    0.262694           Stedelijk Museum Amsterdam        en   \n",
       "5    0.260603               Little Princess statue        en   \n",
       "6    0.254313             Palais de la Porte Dorée        en   \n",
       "7    0.252783                      Vatican Museums        en   \n",
       "8    0.252783                      Vatican Museums        en   \n",
       "9    0.252783                      Vatican Museums        en   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://en.wikipedia.org/wiki/Museum_of_Art_Co...   \n",
       "1  https://en.wikipedia.org/wiki/Museum_of_Decora...   \n",
       "2  https://en.wikipedia.org/wiki/Hungarian_Nation...   \n",
       "3  https://en.wikipedia.org/wiki/Mus%C3%A9e_d%27A...   \n",
       "4  https://en.wikipedia.org/wiki/Stedelijk_Museum...   \n",
       "5  https://en.wikipedia.org/wiki/Little_Princess_...   \n",
       "6  https://en.wikipedia.org/wiki/Palais_de_la_Por...   \n",
       "7      https://en.wikipedia.org/wiki/Vatican_Museums   \n",
       "8      https://en.wikipedia.org/wiki/Vatican_Museums   \n",
       "9      https://en.wikipedia.org/wiki/Vatican_Museums   \n",
       "\n",
       "                         name_monument       city method   note    pageid  \n",
       "0            Museum of Art Collections  Bucharest  title  exact   3201895  \n",
       "1  Museum of Decorative Arts in Prague     Prague  title  exact  23548614  \n",
       "2           Hungarian National Gallery   Budapest  title  exact   7645878  \n",
       "3         Musée d'Art Moderne de Paris      Paris  title  exact  15614518  \n",
       "4           Stedelijk Museum Amsterdam  Amsterdam  title  exact    931845  \n",
       "5               Little Princess Statue   Budapest  title  exact  38595315  \n",
       "6             Palais de la Porte Dorée      Paris  title  exact  17470851  \n",
       "7                      Vatican Museums       Rome  title  exact    229765  \n",
       "8             Museo Gregoriano Profano       Rome  title  exact    229765  \n",
       "9                        Braccio Nuovo       Rome  title  exact    229765  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import wikipedia\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Config (easy to tweak)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # swap as desired\n",
    "MAX_ARTICLE_CHARS = 4000                                 # more text = slower, but often better\n",
    "BATCH_SIZE = 32                                          # embedding batch size\n",
    "# ---------------------------------------------\n",
    "\n",
    "def _fetch_article_text(title: str, lang: Optional[str], max_chars: int) -> str:\n",
    "    \"\"\"Fetch plain text for a page title in a given language (safe truncation).\"\"\"\n",
    "    if lang:\n",
    "        try:\n",
    "            wikipedia.set_lang(lang)\n",
    "        except Exception:\n",
    "            pass  # keep whatever default if lang invalid\n",
    "    try:\n",
    "        page = wikipedia.page(title, auto_suggest=False, preload=False)\n",
    "        return (page.content or \"\")[:max_chars]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def embed_texts(texts: Iterable[str], model: SentenceTransformer, batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Embed a list of texts using Sentence-Transformers (L2-normalized).\"\"\"\n",
    "    # model.encode returns numpy array; normalize for cosine via dot\n",
    "    vecs = model.encode(list(texts), batch_size=batch_size, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "    return vecs\n",
    "\n",
    "def rank_wikipedia_by_query(\n",
    "    df: pd.DataFrame,\n",
    "    query: str = \"world war 2\",\n",
    "    text_col_limit: int = MAX_ARTICLE_CHARS,\n",
    "    model_name: str = EMBED_MODEL,\n",
    "    top_k: Optional[int] = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From an annotated DataFrame (with columns like: name_monument, city, method, match_title, wiki_lang, url),\n",
    "    compute embeddings for entries with method=='title' and rank by cosine similarity to the query phrase.\n",
    "\n",
    "    Returns a new DataFrame with an added 'similarity' column, sorted desc by similarity.\n",
    "    \"\"\"\n",
    "    # Basic checks\n",
    "    needed_cols = {\"method\", \"match_title\", \"wiki_lang\", \"url\"}\n",
    "    missing = needed_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"DataFrame missing required columns: {missing}\")\n",
    "\n",
    "    # Filter to \"title\" hits only\n",
    "    sub = df[df[\"method\"].astype(str).str.lower() == \"title\"].copy()\n",
    "    if sub.empty:\n",
    "        raise ValueError(\"No rows with method == 'title' to score.\")\n",
    "\n",
    "    # Fetch article texts (with simple in-memory cache)\n",
    "    cache = {}\n",
    "    texts = []\n",
    "    titles = []\n",
    "    langs = []\n",
    "    urls = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Fetching article text for {len(sub)} rows...\")\n",
    "\n",
    "    for _, row in tqdm(sub.iterrows(), total=len(sub)):\n",
    "        title = str(row[\"match_title\"])\n",
    "        lang = None if pd.isna(row[\"wiki_lang\"]) else str(row[\"wiki_lang\"])\n",
    "        url  = None if pd.isna(row[\"url\"]) else str(row[\"url\"])\n",
    "\n",
    "        key = (title, lang)\n",
    "        if key not in cache:\n",
    "            cache[key] = _fetch_article_text(title, lang, text_col_limit)\n",
    "        text = cache[key]\n",
    "\n",
    "        titles.append(title)\n",
    "        langs.append(lang)\n",
    "        urls.append(url)\n",
    "        texts.append(text)\n",
    "\n",
    "    sub = sub.assign(_article_text=texts, _title=titles, _lang=langs, _url=urls)\n",
    "\n",
    "    # Drop any truly empty texts (no content could be fetched)\n",
    "    sub = sub[sub[\"_article_text\"].str.len() > 0]\n",
    "    if sub.empty:\n",
    "        raise ValueError(\"All 'title' rows had empty fetched content; cannot rank.\")\n",
    "\n",
    "    # Load embedding model\n",
    "    if verbose:\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "    st_model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Embed documents and the query (normalized)\n",
    "    if verbose:\n",
    "        print(\"Embedding documents...\")\n",
    "    doc_vecs = embed_texts(sub[\"_article_text\"].tolist(), st_model)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Embedding query: '{query}'\")\n",
    "    query_vec = embed_texts([query], st_model)[0]  # shape (dim,)\n",
    "\n",
    "    # Cosine similarity (dot product since vectors are L2-normalized)\n",
    "    sims = doc_vecs @ query_vec\n",
    "\n",
    "    # Attach and sort\n",
    "    ranked = sub.copy()\n",
    "    ranked[\"similarity\"] = sims\n",
    "    ranked = ranked.sort_values(\"similarity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Select useful columns for display\n",
    "    display_cols = [\n",
    "        \"similarity\",\n",
    "        \"match_title\",\n",
    "        \"wiki_lang\",\n",
    "        \"url\",\n",
    "    ]\n",
    "    # Include original monument & city if present\n",
    "    for c in (\"name_monument\", \"city\", \"method\", \"note\", \"pageid\"):\n",
    "        if c in ranked.columns:\n",
    "            display_cols.append(c)\n",
    "\n",
    "    return ranked[display_cols]\n",
    "\n",
    "# -------------------------\n",
    "# Example usage in notebook\n",
    "# -------------------------\n",
    "# Suppose you have `annotated` from the prior step:\n",
    "ranked = rank_wikipedia_by_query(\n",
    "    annotated,\n",
    "    query=\"Art\",                  # <-- flexible: change to any theme, e.g. \"gothic architecture\"\n",
    "    text_col_limit=4000,            # how much article text to embed\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    top_k=None,                     # set an int to slice head()\n",
    ")\n",
    "ranked.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7d73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked.to_csv(\"art_culture_ranked.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KreativHack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
